{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ae2bab",
   "metadata": {},
   "source": [
    "# MobileNetV2 with Synthetic Face Data\n",
    "\n",
    "Dataset from [Kaggle](https://www.kaggle.com/datasets/allexmendes/synthetic-gaze-and-face-segmentation/data)\n",
    "\n",
    "This notebook trains a gaze prediction model using a dual-input CNN:\n",
    "- **Input 1**: 224Ã—224 face image (see the other notebook for preprocessing)\n",
    "- **Input 2**: 4D pupil coordinate vector (`L_Pupil` and `R_Pupil`)\n",
    "- **Output**: 2D normalized gaze direction vector (from `ImageEyesGazeDirection` annotation)\n",
    "\n",
    "We use MobileNetV2 as the backbone for the visual stream and concatenate it with pupil coordinates before regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, Concatenate, GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792394a5",
   "metadata": {},
   "source": [
    "## Load Dataset with Normalized Pupil Coordinates and Gaze Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc114d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/ste/Documents/gesture-gaze-extension/datasets'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'SynthGazeProcessed/images')\n",
    "JSON_DIR = os.path.join(DATA_DIR, 'SynthGazeProcessed/json')\n",
    "IMG_SIZE = (224, 224)\n",
    "SEED = 42028\n",
    "\n",
    "def load_dataset():\n",
    "    images = []\n",
    "    pupils = []\n",
    "    labels = []\n",
    "\n",
    "    for fname in sorted(os.listdir(JSON_DIR)):\n",
    "        if not fname.endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        json_path = os.path.join(JSON_DIR, fname)\n",
    "        img_path = os.path.join(IMG_DIR, fname.replace('.json', '.png'))\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        l_pupil = data[\"Landmarks\"][\"L_Pupil\"]\n",
    "        r_pupil = data[\"Landmarks\"][\"R_Pupil\"]\n",
    "        # Normalize pupil coords\n",
    "        norm_pupils = [\n",
    "            l_pupil[0] / IMG_SIZE, l_pupil[1] / IMG_SIZE,\n",
    "            r_pupil[0] / IMG_SIZE, r_pupil[1] / IMG_SIZE\n",
    "        ]\n",
    "\n",
    "        gaze = data[\"Overall\"][\"ImageEyesGazeDirection\"]\n",
    "\n",
    "        images.append(img)\n",
    "        pupils.append(norm_pupils)\n",
    "        labels.append(gaze)\n",
    "\n",
    "    return np.array(images), np.array(pupils), np.array(labels)\n",
    "\n",
    "X_img, X_pupil, y = load_dual_input_dataset()\n",
    "X_img_train, X_img_val, X_pupil_train, X_pupil_val, y_train, y_val = train_test_split(\n",
    "    X_img, X_pupil, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab97a40",
   "metadata": {},
   "source": [
    "## Define Cosine Similarity Loss for Gaze Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004efcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true = tf.math.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = tf.math.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_sum(y_true * y_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e594e3",
   "metadata": {},
   "source": [
    "## Build the Dual-Input Gaze Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6845173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dual_input_model():\n",
    "    img_input = Input(shape=(224, 224, 3), name='image_input')\n",
    "    pupil_input = Input(shape=(4,), name='pupil_input')\n",
    "\n",
    "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "    y = Dense(32, activation='relu')(pupil_input)\n",
    "\n",
    "    combined = Concatenate()([x, y])\n",
    "    z = Dense(128, activation='relu')(combined)\n",
    "    z = Dropout(0.3)(z)\n",
    "    output = Dense(2, activation='linear', name='gaze_output')(z)\n",
    "\n",
    "    model = Model(inputs=[img_input, pupil_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = build_dual_input_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=cosine_loss, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba378b0",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    [X_img_train, X_pupil_train], y_train,\n",
    "    validation_data=([X_img_val, X_pupil_val], y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7712e59d",
   "metadata": {},
   "source": [
    "## Visualize Gaze Prediction on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(img, vector, color=(0, 255, 0), scale=50):\n",
    "    h, w = img.shape[:2]\n",
    "    start = (w // 2, h // 2)\n",
    "    end = (int(start[0] + vector[0]*scale), int(start[1] + vector[1]*scale))\n",
    "    img_arrow = img.copy()\n",
    "    cv2.arrowedLine(img_arrow, start, end, color, 2, tipLength=0.3)\n",
    "    return img_arrow\n",
    "\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    img = (X_img_val[i] * 255).astype(np.uint8)\n",
    "    true_vec = y_val[i]\n",
    "    pred_vec = model.predict([X_img_val[i:i+1], X_pupil_val[i:i+1]])[0]\n",
    "\n",
    "    vis_img = draw_vector(img, true_vec, color=(0, 255, 0))\n",
    "    vis_img = draw_vector(vis_img, pred_vec, color=(255, 0, 0))\n",
    "\n",
    "    plt.imshow(vis_img)\n",
    "    plt.title(\"Green: GT, Red: Pred\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
