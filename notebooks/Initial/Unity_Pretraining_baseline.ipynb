{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONN+WLSA42Wpjg0c5M5dwZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3glODXmur2HC"},"outputs":[],"source":["# Mount Google Drive to access shared datasets\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import tensorflow as tf\n","\n","# Base of your shared drive\n","ROOT_DIR    = '/content/drive/Shareddrives/OmniClick Team'\n","DATA_DIR    = os.path.join(ROOT_DIR, 'datasets')\n","UNITY_DIR   = os.path.join(DATA_DIR, 'UnityEyes')    # <> where train/val/test live\n","\n","# Hyperparameters\n","IMG_SIZE   = (96, 96)   # all images will be resized to 96×96\n","BATCH_SIZE = 32\n","SEED       = 42028\n","AUTOTUNE   = tf.data.AUTOTUNE\n","EPOCHS     = 100        # we’ll rely on EarlyStopping to halt earlier\n"]},{"cell_type":"code","source":["# 1) Load folder-structured datasets\n","unity_train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    os.path.join(UNITY_DIR, 'train'),\n","    labels='inferred', label_mode='categorical',\n","    image_size=IMG_SIZE, batch_size=BATCH_SIZE,\n","    shuffle=True, seed=SEED\n",")\n","unity_val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    os.path.join(UNITY_DIR, 'val'),\n","    labels='inferred', label_mode='categorical',\n","    image_size=IMG_SIZE, batch_size=BATCH_SIZE,\n","    shuffle=False\n",")\n","unity_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    os.path.join(UNITY_DIR, 'test'),\n","    labels='inferred', label_mode='categorical',\n","    image_size=IMG_SIZE, batch_size=BATCH_SIZE,\n","    shuffle=False\n",")\n","\n","# 2) Normalize pixels and prepare pipeline\n","def preprocess(image, label):\n","    \"\"\"Scale image pixels to [0,1].\"\"\"\n","    image = tf.cast(image, tf.float32) / 255.0\n","    return image, label\n","\n","unity_train_ds = (\n","    unity_train_ds\n","    .map(preprocess, num_parallel_calls=AUTOTUNE)\n","    .cache()\n","    .shuffle(1000, seed=SEED)\n","    .prefetch(AUTOTUNE)\n",")\n","unity_val_ds = (\n","    unity_val_ds\n","    .map(preprocess, num_parallel_calls=AUTOTUNE)\n","    .cache()\n","    .prefetch(AUTOTUNE)\n",")\n","unity_test_ds = (\n","    unity_test_ds\n","    .map(preprocess, num_parallel_calls=AUTOTUNE)\n","    .cache()\n","    .prefetch(AUTOTUNE)\n",")\n","\n","# 3) Determine number of Unity class labels\n","NUM_CLASSES_UNITY = len(unity_train_ds.class_names)\n","print(\"UnityEyes classes:\", unity_train_ds.class_names)\n"],"metadata":{"id":"eWQ3rFDTr9CN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import layers, models, Input\n","\n","def build_baseline(num_classes):\n","    \"\"\"\n","    Build a simple custom CNN:\n","    Conv(32)→MaxPool→Conv(64)→MaxPool→Conv(128)→GAP→Dense(128)→Dropout(0.3)→Softmax\n","    \"\"\"\n","    return models.Sequential([\n","        Input(shape=(*IMG_SIZE, 3)),\n","        layers.Conv2D(32, 3, activation='relu'),\n","        layers.MaxPooling2D(),\n","        layers.Conv2D(64, 3, activation='relu'),\n","        layers.MaxPooling2D(),\n","        layers.Conv2D(128, 3, activation='relu'),\n","        layers.GlobalAveragePooling2D(),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dropout(0.3),\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","\n","# Instantiate the model for Unity pretraining\n","model = build_baseline(NUM_CLASSES_UNITY)\n","model.summary()\n"],"metadata":{"id":"yLfdJWJKr8-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# Compile with Adam optimizer and categorical crossentropy\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# EarlyStopping and LR reduction on plateau\n","callbacks = [\n","    EarlyStopping(\n","        monitor='val_loss',\n","        patience=10,\n","        restore_best_weights=True,\n","        verbose=1\n","    ),\n","    ReduceLROnPlateau(\n","        monitor='val_loss',\n","        factor=0.5,\n","        patience=5,\n","        verbose=1\n","    )\n","]\n"],"metadata":{"id":"B5TgHGyor87a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train up to EPOCHS=100, but stop early when val_loss plateaus\n","history_unity = model.fit(\n","    unity_train_ds,\n","    validation_data=unity_val_ds,\n","    epochs=EPOCHS,\n","    callbacks=callbacks\n",")\n"],"metadata":{"id":"THNyO6S3r81v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) Save weights for later fine-tuning\n","WEIGHTS_PATH = os.path.join(ROOT_DIR, 'notebooks/weights/unity_pretrained.h5')\n","model.save_weights(WEIGHTS_PATH)\n","print(\"Saved Unity pretraining weights to:\", WEIGHTS_PATH)\n","\n","# 2) Evaluate on Unity test set (optional)\n","loss_u, acc_u = model.evaluate(unity_test_ds)\n","print(f\"Unity Test Loss: {loss_u:.4f}, Accuracy: {acc_u:.4f}\")\n"],"metadata":{"id":"dJP0XLqir8s3"},"execution_count":null,"outputs":[]}]}